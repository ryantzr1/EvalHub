// // src/data/taskCategories.ts
// export const taskCategories: { [key: string]: string[] } = {
//     "Language Understanding": ["aclue", "anli", "blimp", "ceval", "glue", "siqa", "super_glue"],
//     "Mathematical and Logical Reasoning": ["arithmetic", "asdiv", "gsm8k", "hendrycks_math (MATH)", "logiqa", "mathqa", "minerva_math"],
//     "Question Answering": ["aexams", "ARC, AI2 Reasoning Challenge", "babi", "coqa", "csatqa", "drop", "headqa", "lambada", "medmcqa", "medqa", "nq_open", "openbookqa", "pubmedqa", "qa4mre", "qasper", "race", "sciq", "squad_completion", "squadv2", "triviaqa", "truthfulqa"],
//     "Commonsense Reasoning": ["agieval", "commonsense_qa", "hellaswag", "piqa", "winogrande"],
//     "Cross-lingual and Multilingual Tasks": ["basqueglue", "belebele", "cmmlu", "eus_exams", "eus_proficiency", "eus_reading", "eus_trivia", "kobest", "kormedmcqa", "lambada_multilingual", "lambada_multilingual_stablelm", "mgsm", "okapi/arc_multilingual", "okapi/hellaswag_multilingual", "okapi/mmlu_multilingual", "okapi/truthfulqa_multilingual", "paws-x", "translation", "xcopa", "xnli", "xnli_eu", "xstorycloze", "xwinograd"],
//     "Ethics and Bias": ["crows_pairs", "eq_bench", "hendrycks_ethics", "realtoxicityprompts", "toxigen"],
//     "Specialized Domains": ["code_x_glue", "copal_id", "fda", "headqa", "ifeval", "logiqa2", "prost", "sciq", "tmmluplus", "toxigen"],
//     "Miscellaneous": ["bigbench", "benchmarks", "bbh", "gpt_3", "mmlu", "model_written_evals", "pile", "pile_10k", "scrolls", "swde", "tinyBenchmarks", "unitxt"]
//   };
  